# GÂ³TOD

**GÂ³TOD** is a generalizable framework designed to enhance personalization in task-oriented dialogue systems. It utilizes three structured knowledge graphs extracted from conversation history:

- ğŸ§¾ **Entity Context Graph**  
- ğŸ‘¤ **User Persona Graph**  
- ğŸ§  **Commonsense Reasoning Graph**

These graphs help model rich, user-aware, and context-driven interactions, improving response relevance and coherence.

---

# ğŸ§  Triplet Extraction System

This repository implements the triplet extraction module for G3TOD. It extracts (subject, relation, object) triplets from natural language using entity recognition and ConceptNet-based reasoning. These triplets are crucial for building knowledge graphs that support personalized dialogue systems, semantic analysis, and downstream NLP tasks.

---

## ğŸ“ Project Structure

â”œâ”€â”€ conceptnet.py # Interface for querying ConceptNet

â”œâ”€â”€ entity.py # Entity recognition and cleaning logic

â”œâ”€â”€ extract_rel_triplets.py # Core logic for relationship extraction

â”œâ”€â”€ triplet_extract.py # Utilities for triplet formatting and filtering

â”œâ”€â”€ main.py # Script for sample triplet extraction

â”œâ”€â”€ run.py # Entry point to execute the extraction pipeline

â”œâ”€â”€ test.py # Unit tests for each component

â”œâ”€â”€ requirements.txt # List of required Python packages

â””â”€â”€ README.md # Project documentation

yaml
Copy
Edit

---

## ğŸš€ Installation

### 1. Clone the Repository
    ```bash
    git clone https://github.com/yourusername/triplet-extraction.git
    cd triplet-extraction


### 2. Create and Activate a Virtual Environment
     ```bash
     python -m venv venv
    source venv/bin/activate        # On Windows: venv\Scripts\activate

#### ğŸ¤– Ollama Setup (if not already)
      ```bash
      1. Install Ollama
      Follow the official instructions here: https://ollama.com/download

      2. Start Ollama
      ollama serve
      3. Pull a Model (e.g., LLaMA2 or openai)
     ollama pull llama2
     # or
     ollama pull openai

### 3. Install Dependencies
      ```bash
      pip install -r requirements.txt

### 4. Run the Extraction Pipeline
     ```bash
      ğŸ› ï¸ Usage
         
    python run.py
    This processes a predefined input (from main.py). You can modify main.py to process:
    Run Unit Tests
    python test.py
This will validate core functionalities such as entity recognition, ConceptNet query interface, and triplet extraction accuracy.

ğŸ“¦ Dependencies
spacy â€“ Entity recognition and NLP processing

nltk â€“ Tokenization, POS tagging, and basic NLP tools

requests â€“ Querying ConceptNet API

networkx â€“ For handling and visualizing graph relations

See requirements.txt for exact versions.
